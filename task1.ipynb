{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36564bitanaconda3conda4d2a131d00244148923b6e8eafe61e3b",
   "display_name": "Python 3.6.5 64-bit ('Anaconda3': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "PhraseId  SentenceId                                             Phrase  Sentiment\n0              1           1  A series of escapades demonstrating the adage ...          1\n1              2           1  A series of escapades demonstrating the adage ...          2\n2              3           1                                           A series          2\n3              4           1                                                  A          2\n4              5           1                                             series          2\n...          ...         ...                                                ...        ...\n156055    156056        8544                                          Hearst 's          2\n156056    156057        8544                          forced avuncular chortles          1\n156057    156058        8544                                 avuncular chortles          3\n156058    156059        8544                                          avuncular          2\n156059    156060        8544                                           chortles          2\n\n[156060 rows x 4 columns]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "52"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_root = './data/'\n",
    "train_data=pd.read_csv(data_root+'train.tsv',sep='\\t')\n",
    "test_data=pd.read_csv(data_root+'test.tsv',sep='\\t')\n",
    "pd.set_option('display.width', 900)\n",
    "print(train_data)\n",
    "\n",
    "Phrases = train_data['Phrase']\n",
    "MaxSeqlen = 0\n",
    "for p in Phrases:\n",
    "    words = p.split()\n",
    "    if len(words) > MaxSeqlen:\n",
    "        MaxSeqlen = len(words)\n",
    "MaxSeqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0    A series of escapades demonstrating the adage ...\n1    A series of escapades demonstrating the adage ...\n2                                             A series\n3                                                    A\n4                                               series\nName: Phrase, dtype: object\n"
    },
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 21.2 GiB for an array with shape (156060, 18226) and data type float64",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-115571720126>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mword2id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mBOW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPhrases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPhrases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 21.2 GiB for an array with shape (156060, 18226) and data type float64"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "word2id = {}\n",
    "# get word-index dictionary\n",
    "for i,p in enumerate(Phrases):\n",
    "    words = p.split()\n",
    "    for word in words:\n",
    "        if word not in word2id:\n",
    "            word2id[word]=len(word2id)\n",
    "\n",
    "BOW = np.zeros((len(Phrases),len(word2id)), dtype=np.float32)\n",
    "\n",
    "for i,p in enumerate(Phrases):\n",
    "    words = p.split()\n",
    "    for word in words:\n",
    "        BOW[i][word2id[word]] += 1\n",
    "\n",
    "print(BOW[:20])\n",
    "labels = train_data['Sentiment']\n",
    "label_list = list(set(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-94a03cb497cc>, line 3)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-94a03cb497cc>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    n_features =len(word2id)n\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "n_iter = 10\n",
    "batch_size = 128# \n",
    "n_features =len(word2id)n\n",
    "n_class = len(label_list)\n",
    "lr = 1e-4\n",
    "print_every = 500\n",
    "\n",
    "def make_network(n_hidden=128):\n",
    "    model = dict(\n",
    "        W1=np.random.randnword2idn, n_hidden),\n",
    "        W2=np.random.randn(n_hidden, n_class)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/(np.sum(np.exp(x),axis=1)[:,None])\n",
    "\n",
    "def forward(x,w):\n",
    "    h = x @ w['W1']\n",
    "   #  h[h<0] = 0\n",
    "    x = h @ w['W2']\n",
    "    output = softmax(x)\n",
    "    return h, output\n",
    "\n",
    "def ce_loss(probs, labels):\n",
    "    log_probs = np.log(probs)\n",
    "    return -np.sum(log_probs[:,labels])\n",
    "\n",
    "def backward(train_data, h, w, output):\n",
    "    err = output - 1\n",
    "    dW2 = h.T @ err\n",
    "    dh = err @ w['W2'].T\n",
    "   #  dh[h<0] = 0\n",
    "    dW1 = train_data.T @ dh\n",
    "    return dict(W1=dW1, W2=dW2)\n",
    "\n",
    "def sgd_step(w, grad_w):\n",
    "    for param in w:\n",
    "        w[param] -= lr*grad_w[param]\n",
    "\n",
    "model = make_network()\n",
    "step = 0\n",
    "for i in range(n_iter):\n",
    "    start_idx = 0\n",
    "    np.random.shuffle(BOW)\n",
    "    while start_idx + batch_size <= BOW.shape[0]:\n",
    "        train_batch = BOW[start_idx:start_idx+batch_size]\n",
    "        train_label = labels[start_idx:start_idx+batch_size]\n",
    "        h, output = forward(train_batch, model)\n",
    "        loss = ce_loss(output, train_label)\n",
    "        model_grad = backward(train_batch, h, model, output)\n",
    "        print(model['W1'][0][:10], model_grad['W1'][0][:10])\n",
    "        sgd_step(model, model_grad)\n",
    "        if step%print_every == 0:\n",
    "            print(\"Step %d, loss: %.2f\"%(step,loss))\n",
    "        step += 1\n",
    "        start_idx += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}